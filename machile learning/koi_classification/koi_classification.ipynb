{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático de Machine Learning\n",
    "#### Aluna: Cecília Regina Oliveira de Assis - 2019697720\n",
    "#### Professor: Adriano Veloso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacoes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import model_selection \n",
    "from sklearn import naive_bayes \n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho do arquivo de entrada\n",
    "INPUT_FILEPATH = 'koi_data.csv'\n",
    "\n",
    "# Variavel de predicao\n",
    "TARGET = 'koi_disposition'\n",
    "\n",
    "# Label das features que nao serao utilizadas no treinamento dos modelos\n",
    "NOT_USED = [TARGET, 'kepoi_name']\n",
    "\n",
    "# Número de splits da validacao cruzada (cross validation - CV)\n",
    "K_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto de dados usado neste notebook corresponde a características de candidatos a exoplanetas (KOI - Kepler Object of Interest) enumeradas a partir de leituras realizadas pela sonda espacial Kepler. \n",
    "\n",
    "A primeira coluna da base de dados identifica o KOI enquanto a segunda a traz a sua classificação (FALSE POSITIVE ou CONFIRMED). As demais colunas são features sobre o KOI extraídas de diversas formas.\n",
    "\n",
    "Fonte dos dados: [Kepler Exoplanet Search Results](https://www.kaggle.com/nasa/kepler-exoplanet-search-results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Leitura do dataset para um DataFrame do Pandas\n",
    "# usando separador default (virgula)\n",
    "df = pd.read_csv(INPUT_FILEPATH)\n",
    "\n",
    "# Cria uma lista com as features\n",
    "features = list(df.columns)\n",
    "for x in NOT_USED:\n",
    "    features.remove(x)\n",
    "\n",
    "# Imprime metadados do conjunto\n",
    "print('Numero de linhas: {}'.format(df.shape[0]))\n",
    "print('Numero de colunas: {}'.format(df.shape[1]))\n",
    "print('Dados faltando: {}'.format(df.isnull().sum().sum()))\n",
    "\n",
    "# Exibe a proporcao de cada classe\n",
    "print('\\nProporcao de cada classe:')\n",
    "display((df[TARGET].value_counts()/len(df)*100).round(2).to_frame(TARGET +\" (%)\").T)\n",
    "\n",
    "# Apresenta um breve sumário do conjunto\n",
    "print('\\nSumario do conjunto de dados:')\n",
    "display(df[features].describe()) \n",
    "\n",
    "print('\\nAmostra:')\n",
    "with pd.option_context('max_columns', 8):\n",
    "    display(df.head(20))\n",
    "    \n",
    "print('Target: {}'.format(TARGET))\n",
    "print('Features:')\n",
    "print('\\n'.join(['  ' + x for x in features]))\n",
    "\n",
    "mpl.rc('font', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[TARGET] = (df[TARGET] == 'CONFIRMED').astype(int)\n",
    "\n",
    "print('Resultado:')\n",
    "\n",
    "display(df[[TARGET]].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reacria a lista com as features\n",
    "features = list(df.columns)\n",
    "for x in NOT_USED:\n",
    "    features.remove(x)\n",
    "\n",
    "# Normaliza os atributos usando a metodologia MinMax\n",
    "#   Define o intervalo de cada feature\n",
    "minmax = df[features].max() - df[features].min()\n",
    "\n",
    "#   Subtrai o mínimo valor da feature e divide pelo intervalo \n",
    "df[features] = (df[features] - df[features].min()) / minmax\n",
    "\n",
    "display(df[features].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apresentação gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define as variaveis que armazenam o melhor modelo por metrica\n",
    "best_ein             = ['', 1.01]\n",
    "best_eout            = ['', 1.01]\n",
    "best_precision_false = ['', -1]\n",
    "best_precision_true  = ['', -1]\n",
    "best_recall_false    = ['', -1]\n",
    "best_recall_true     = ['', -1]\n",
    "\n",
    "# Define uma funcao que atualiza o melhor modelo por metrica\n",
    "def update_best(title, model):\n",
    "    if model['train_acc'] < best_ein[1]:\n",
    "        best_ein[1] = results['train_acc']\n",
    "        best_ein[0] = title\n",
    "    \n",
    "    if model['test_acc'] < best_eout[1]:\n",
    "        best_eout[1] = model['test_acc']\n",
    "        best_eout[0] = title\n",
    "        \n",
    "    if model['precision'][0] > best_precision_false[1]:\n",
    "        best_precision_false[1] = model['precision'][0]\n",
    "        best_precision_false[0] = title\n",
    "        \n",
    "    if model['precision'][1] > best_precision_true[1]:\n",
    "        best_precision_true[1] = model['precision'][1]\n",
    "        best_precision_true[0] = title\n",
    "        \n",
    "    if model['recall'][0] > best_recall_false[1]:\n",
    "        best_recall_false[1] = model['recall'][0]\n",
    "        best_recall_false[0] = title\n",
    "        \n",
    "    if model['recall'][1] > best_recall_true[1]:\n",
    "        best_recall_true[1] = model['recall'][1]\n",
    "        best_recall_true[0] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define uma funcao que avalia se o valor passado eh None ou nao\n",
    "def xstr(s):\n",
    "    return 'None' if s is None else str(s)\n",
    "\n",
    "# Define uma funcao que apresenta graficamente o \n",
    "# resultado (train_values, test_values) do classificador (title)\n",
    "# Os valores do hiperparametro e sua definicao eh recebido\n",
    "# pelas variaveis params e xlabel, respectivamente\n",
    "def plot_values(title, xlabel, params, results):\n",
    "    # Inicializa grafico\n",
    "    fig = plt.figure(figsize=(19,5)) \n",
    "    ax  = fig.add_subplot(111)\n",
    "\n",
    "    # Define titulo do grafico\n",
    "    fig.suptitle(title, fontsize=20, fontweight='bold')\n",
    "    \n",
    "    for idx, value in enumerate(results):\n",
    "        # Define valores a serem exibidos\n",
    "        param   = xstr(params[idx])\n",
    "        v_train = value['train_acc']\n",
    "        v_test  = value['test_acc']\n",
    "        \n",
    "        ax.scatter(param, v_train, color='orange')\n",
    "        ax.scatter(param, v_test,  color='blue')\n",
    "        \n",
    "        # Define o label dos pontos plotados\n",
    "        ax.annotate('  %.3f' % v_train, xy=(param, v_train))\n",
    "        ax.annotate('  %.3f' % v_test, xy=(param, v_test))\n",
    "        \n",
    "    # Define rotulo dos eixos x e y\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel('Accurary')\n",
    "    \n",
    "    # Exibte o grafico\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define uma funcao que plota as curvas roc de um dado modelo\n",
    "def plot_roc_curve(title, clf):\n",
    "    folds = len(train_idx_folds)\n",
    "        \n",
    "    # Inicializa grafico\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax  = fig.add_subplot(111)\n",
    "    \n",
    "    # Define titulo do grafico\n",
    "    fig.suptitle('Roc Curve  - {}'.format(title), \n",
    "                 fontsize=20, fontweight='bold')\n",
    "    \n",
    "    for i in range(folds):\n",
    "        # Recupera os índices dos conjuntos de treino e teste\n",
    "        x_train = df.loc[train_idx_folds[i], features]\n",
    "        y_train = df.loc[train_idx_folds[i], TARGET]\n",
    "        x_test  = df.loc[test_idx_folds[i],  features]\n",
    "        y_test  = df.loc[test_idx_folds[i],  TARGET]\n",
    "        \n",
    "        # Treina e testa o modelo\n",
    "        model = clf.fit(x_train, y_train)\n",
    "        \n",
    "        # Calcula e armazena a revocacao e a precisao\n",
    "        y_proba = clf.predict_proba(x_test)[:, 1]\n",
    "        \n",
    "        # Calcula as taxas de falso positivo e negativo para a curva roc\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_test, y_proba)\n",
    "\n",
    "        # Define valores a serem exibidos\n",
    "        ax.plot(fpr, tpr, label='Run {}'.format(i + 1))\n",
    "        ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "\n",
    "        # Configura os limites dos eixos\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    \n",
    "    # Define rotulo dos eixos\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    \n",
    "    # Habilita a exibicao e legendas\n",
    "    ax.legend()\n",
    "    \n",
    "    # Exibte o grafico\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cria o gerador de k-folds\n",
    "kf = model_selection.KFold(n_splits=K_FOLDS)\n",
    "\n",
    "train_idx_folds = []\n",
    "test_idx_folds  = []\n",
    "for train_idx, test_idx in kf.split(df):\n",
    "    train_idx_folds.append(train_idx)\n",
    "    test_idx_folds.append(test_idx)\n",
    "\n",
    "# Imprime breve sumário dos k-folds\n",
    "folds = len(train_idx_folds)\n",
    "\n",
    "print('Quantidade de folds (k): {}'.format(folds))\n",
    "print('Folds:')\n",
    "print('\\tTamanho folds de treino: {}'.format(len(train_idx_folds[0])))\n",
    "print('\\tTamanho folds de teste: {}\\n'.format(len(test_idx_folds[0])))\n",
    "for i in range(folds):\n",
    "    print('\\tTreino: {}'.format(train_idx_folds[i]))\n",
    "    print('\\tTeste:  {}\\n'.format(test_idx_folds[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define uma funcao que treina e testa o classificador clf,\n",
    "# sobre o conjunto de dados df\n",
    "def run_classifier(df, clf):\n",
    "    folds      = len(train_idx_folds)\n",
    "\n",
    "    train_acc = 0\n",
    "    test_acc  = 0\n",
    "    precision = []\n",
    "    recall    = []\n",
    "    \n",
    "    # Para cada fold:\n",
    "    for i in range(folds):\n",
    "        # Recupera os índices dos conjuntos de treino e teste\n",
    "        x_train = df.loc[train_idx_folds[i], features]\n",
    "        y_train = df.loc[train_idx_folds[i], TARGET]\n",
    "        x_test  = df.loc[test_idx_folds[i],  features]\n",
    "        y_test  = df.loc[test_idx_folds[i],  TARGET]\n",
    "        \n",
    "        # Treina e testa o modelo\n",
    "        model = clf.fit(x_train, y_train)\n",
    "        \n",
    "        # Armazenas acuracias\n",
    "        train_acc += model.score(x_train, y_train)\n",
    "        test_acc  += model.score(x_test, y_test)\n",
    "        \n",
    "        # Calcula e armazena a revocacao e a precisao\n",
    "        y_pred = clf.predict(x_test)\n",
    "        \n",
    "        p = metrics.precision_score(y_test, y_pred, average=None)\n",
    "        r = metrics.recall_score(y_test, y_pred, average=None)\n",
    "        \n",
    "        precision.append(p) \n",
    "        recall.append(r)\n",
    "        \n",
    "    # Calcula e armazena as medias de acuracia, precisao e revocacao\n",
    "    train_acc /= folds\n",
    "    test_acc  /= folds\n",
    "    precision  = np.asarray(precision).mean(axis=0)\n",
    "    recall     = np.asarray(recall).mean(axis=0)\n",
    "    \n",
    "    return {\n",
    "        'train_acc': train_acc, \n",
    "        'test_acc': test_acc, \n",
    "        'precision': precision, \n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apresenta o valor das métricas apuradas pelo classificador\n",
    "def show_metrics(title, clf):\n",
    "    v_test  = clf['test_acc']\n",
    "    v_train = clf['train_acc']\n",
    "    \n",
    "    print('\\t\\tMetrics - {}\\n'.format(title))    \n",
    "    print('Erros:')\n",
    "    print('\\tEmpírico: %.3f' % (1 - v_train))\n",
    "    print('\\tEsperado: %.3f' % (1 - v_test))\n",
    "    print('\\tDiferença de aproximação: %.3f' % abs(v_train - v_test))\n",
    "    \n",
    "    print('Precisao:')\n",
    "    print('\\tFalse Positive (No):  %.3f' % clf['precision'][0])\n",
    "    print('\\tConfirmed      (Yes): %.3f' % clf['precision'][1])  \n",
    "\n",
    "    print('Revocacao:')\n",
    "    print('\\tFalse Positive (No):  %.3f' % clf['recall'][0])\n",
    "    print('\\tConfirmed      (Yes): %.3f\\n' % clf['recall'][1])  \n",
    "    \n",
    "    # Atualiza o melhor modelo para cada metrica\n",
    "    update_best(title, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-Bayes (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = 'Naive-Bayes'\n",
    "gnb   = naive_bayes.GaussianNB()\n",
    "\n",
    "# Treina, testa o modelo e apresenta as metricas apuradas\n",
    "show_metrics(title, run_classifier(df=df, clf=gnb))\n",
    "\n",
    "# Apresenta dos resultados\n",
    "plot_roc_curve(title, gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O _baseline_ do experimento foi o algoritmo __Naive-Bayes__, um método probabilístico, não parametrizado, que assume uma distribuição gaussiana dos dados e a independência condicional das variáveis.\n",
    "\n",
    "Por não definir hiperparâmetros, o classificador apura resultados satisfatórios com quantidade pequenas de dados. Tal situação ocorreu no presente experimento, tendo o algoritmo apresentado uma boa aproximação entre erro empírico ($E_{in}$) e erro esperado ($E_{out}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define os valores a serem testados no hiperparametro\n",
    "values  = [None, 2, 5, 10, 20, 30, 40, 50, int(df.shape[0]/100), 100]\n",
    "results = []\n",
    "\n",
    "# Testa cada valor no hiperparametro e apresenta as metricas apuradas\n",
    "for value in values:\n",
    "    title = 'Decision Tree (maxdepth={})'.format(value)\n",
    "    dtree = tree.DecisionTreeClassifier(max_depth=value)\n",
    "    dtree = run_classifier(df=df, clf=dtree)\n",
    "    show_metrics(title, dtree)\n",
    "    \n",
    "    results.append(dtree)\n",
    "\n",
    "# Demonstra graficamente os resultados da Arvore de Decisao\n",
    "title  = 'Decision Tree'\n",
    "xlabel = 'Tree max depth'\n",
    "\n",
    "# Apresenta dos resultados\n",
    "plot_values(title, xlabel, values, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O hiperparâmetro variado para esse algoritmo foi o \"max_depth\" que define a altura máxima da árvore. Dez valores foram utilizados, sendo um deles correspondente a altura infinita, ou segundo aborda o _scikit learn_, até que todas as folhas sejam puras ou não apresentem mais de 2 exemplos (valor padrão para o hiperparâmetro \"min_samples\").\n",
    "\n",
    "Com a variação de tal hiperparâmetro pode-se observar que todos os modelos apresentaram _overfit_ que é o a situação onde o modelo decora os exemplos de treino, não performando bem a generalização. \n",
    "\n",
    "Ainda com o _overfitting_ aquele modelo que se sobressaiu dentre os demais, demonstrando melhor aproximação de erro empírico com erro esperado, foi aquele com altura máxima igual a 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define os valores a serem testados no hiperparametro\n",
    "values  = ['linear', 'sigmoid', 'poly', 'rbf']\n",
    "results = []\n",
    "\n",
    "# Testa cada valor no hiperparametro e apresenta as metricas apuradas\n",
    "for value in values:\n",
    "    title  = 'Support Vector Machine (kernel={})'.format(value)\n",
    "    s = svm.SVC(gamma='auto', kernel=value)\n",
    "    s = run_classifier(df=df, clf=s)\n",
    "    show_metrics(title, s)\n",
    "    \n",
    "    results.append(s)\n",
    "\n",
    "# Demonstra graficamente os resultados do SVM\n",
    "title  = 'Support Vector Machine'\n",
    "xlabel = 'Type of kernel'\n",
    "\n",
    "# Apresentacao dos resultados\n",
    "plot_values(title, xlabel, values, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o algoritmo __SVM__, quatro tipos de kernel (hiperparâmetro), funções que mudam a representação do dado, foram utilizados.\n",
    "\n",
    "A escolha de hiperparâmetro que se saiu melhor foi aquela que adotou o kernel \"linear\". Em contrapartida, o kernel \"polinomial\", que representa as features como polinômios, apresentou características de _underfitting_ ao não generalizar suficientemente o modelo tanto para os dados de treino quanto para os teste. \n",
    "\n",
    "É importante notar que o kernel \"polinomial\" nos dois primeiros folds de teste predisse a classe False Positive (0 ou No) para todos os exemplos, levantando um aviso da biblioteca quanto ao cálculo das métricas (precisão e revocação). Tal comportamento teve por consequência baixa taxa de precisão do modelo para a classe Confirmed (1 ou Yes). \n",
    "\n",
    "Por fim, o hiperparâmetro \"C\", que define o tradeoff entre margem e erro, não foi configurado no experimento atual sendo o valor padrão (\"C=0.01\") do scikit learn adotado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define os valores a serem testados no hiperparametro\n",
    "values  = [2, 5, 10, 20, 30, 40, 50, int(df.shape[0]/100), 100, 250, 300]\n",
    "results = []\n",
    "\n",
    "# Testa cada valor no hiperparametro e apresenta as metricas apuradas\n",
    "for value in values:\n",
    "    title = 'k Nearest Neighbor (n_neighbors={})'.format(value)\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=value)\n",
    "    knn = run_classifier(df=df, clf=knn)\n",
    "    show_metrics(title, knn)\n",
    "    \n",
    "    results.append(knn)\n",
    "\n",
    "# Demonstra graficamente os resultados do kNN\n",
    "title  = 'k Nearest Neighbor'\n",
    "xlabel = 'Number of neighbors'\n",
    "\n",
    "plot_values(title, xlabel, values, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o algoritmo __kNN__ o hiperparâmetro variado foi o número de vizinhos próximos a serem analisados, denotado por \"k\". Todas as tentativas de variação de \"k\" apresentaram _overfitting_ ou _underfitting_, sendo \"k=2\" o mais expressivo dentre aqueles exibiram _overfitting_.\n",
    "\n",
    "O melhor modelo foi aquele com \"k=40\", exibindo diferença entre erros de 0.64. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define os valores a serem testados no hiperparametro\n",
    "values  = [2, 5, 10, 20, 30, 40, 50, int(df.shape[0]/100), 100, 250, 300]\n",
    "results = []\n",
    "\n",
    "# Testa cada valor no hiperparametro e apresenta as metricas apuradas\n",
    "for value in values:\n",
    "    title = 'Random Forest (n_estimators={})'.format(value)\n",
    "    rf = ensemble.RandomForestClassifier(n_estimators=value)\n",
    "    rf = run_classifier(df=df, clf=rf)\n",
    "    show_metrics(title, rf)\n",
    "    \n",
    "    results.append(rf)\n",
    "\n",
    "# Demonstra graficamente os resultados do Random Forest\n",
    "title  = 'Random Forest'\n",
    "xlabel = 'Number of trees'\n",
    "plot_values(title, xlabel, values, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo de _bagging_ __random forest__ teve seu número de árvores, também chamadas de estimadores, variado. A técnica adotada pelo _bagging_ ataca o erro de variância, ao separar os dados de treino ($D$) em $i$ conjuntos, treinar $i$ modelos para cada $D_i$ partição e adotar a média das predições, metodologia esta muito similar a validação cruzada.\n",
    "\n",
    "Ao variar o número de árvores, $i$ da explicação anterior, foi interessante observar que a partir de \"n_estimators=40\" as métricas do classificador variaram minimamente, indicando que tal valor é aquele que leva a convergência do método.\n",
    "\n",
    "De forma similar as demais técnicas até então abordadas, principalmente a árvore de decisão, os modelos elucidados pelo classificador exibiram indícios de _overfitting_, ocorrendo empate de performance a partir de \"n_estimators=40\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define os valores a serem testados no hiperparametro\n",
    "values = [0.02, 0.05, 0.1, 2, 5, 10, 20, 30, 40, 50, int(df.shape[0]/100)]\n",
    "for i in range(len(values)):\n",
    "    values[i] = int(values[i] * 100)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Testa cada valor no hiperparametro e apresenta as metricas apuradas\n",
    "for value in values:\n",
    "    title = 'Gradient Tree Boosting (n_estimators={})'.format(value)\n",
    "    gtb = ensemble.GradientBoostingClassifier(n_estimators=value)\n",
    "    gtb = run_classifier(df=df, clf=gtb)\n",
    "    show_metrics(title, gtb)\n",
    "    \n",
    "    results.append(gtb)\n",
    "\n",
    "# Demonstra graficamente os resultados da Gradient Tree Boosting\n",
    "title  = 'Gradient Tree Boosting'\n",
    "xlabel = 'Number of estimators'\n",
    "plot_values(title, xlabel, values, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, um algoritmo de _boosting_ participou do experimento. A metodologia do _boosting_, no geral, busca associar modelos simples para gerar um classificador bastante capacitado. O __gradient tree boosting__ opta por mesclar árvores de regressão, buscando a minimização de sua função de perda que, por sua vez, leva em conta o desvio dos exemplos.\n",
    "\n",
    "Aqui, o número de árvores (\"n_estimators\") também foi alternado e todos os valores multiplicados por 100. Três novas possiblidades foram adicionadas com o intuito de analisar uma recomendação feita pela própria biblioteca de aprendizado que explicita que um número maior de estimadores tende a levar a uma melhor performance do algoritmo, devido a robustez a _overfitting_ do mesmo, sendo 100 o valor padrão aplicado pelo scikit.\n",
    "\n",
    "A análise das métricas demonstrou que a partir de \"n_estimators=10\" o algoritmo já atinge sua convergência, tendo sua diferença entre erro empírico e erro estimado estabilizado. Além disso, quando o hiperparâmetro assumiu valor igual 10 o método apresentou seu melhor resultado de aproximação. \n",
    "\n",
    "Também foi possível observar que para um número baixo de estimadores o classificador de fato não performa tão bem chegando a predizer somente uma classe para todos os exemplos, contudo um número tão grande quanto 100 estimadores não se mostra significativamente necessário, levando em conta a base de dados em questão. \n",
    "\n",
    "Por fim, em conformidade ao apontado pela ferramenta, aqueles modelos que foram testados com maiores quantidades de estimadores apresentaram _overfitting_ controlado, não ultrapassando 0.03 pontos percentuais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação dos Melhores Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a comparação dos resultados, somente as __curvas roc__ dos melhores modelos serão aprensentadas, visto que os valores das métricas para cada um dos experimentos se encontram acima. \n",
    "\n",
    "Abaixo as curvas, para cada _fold_ da base de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apresenta a curva roc do melhor modelo Arvore de Decisao\n",
    "plot_roc_curve('Decision Tree', tree.DecisionTreeClassifier(max_depth=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apresenta a curva roc do melhor modelo SVM\n",
    "plot_roc_curve('Support Vector Machine', svm.SVC(gamma='auto', \n",
    "                              kernel='linear', probability=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apresenta a curva roc do melhor modelo KNN\n",
    "plot_roc_curve('Random Forest', \n",
    "               ensemble.RandomForestClassifier(n_estimators=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apresenta a curva roc do melhor modelo KNN\n",
    "plot_roc_curve('k Nearest Neighbor', \n",
    "               neighbors.KNeighborsClassifier(n_neighbors=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apresenta a curva roc do melhor modelo Gradient Tree Boosting\n",
    "plot_roc_curve('Gradient Tree Boosting', \n",
    "               ensemble.GradientBoostingClassifier(n_estimators=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "##### Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __curva roc__ é responsável por apresentar uma forma clara do quão bem o modelo se saiu em sua classifcação, utilizando as taxas de verdadeiro positivo, ou seja, exemplos que pertenciam a uma categoria e de fato foram associados a ela e taxa de falsos positivos, que define o caso contrário, onde o algoritmo errou sua predição. \n",
    "\n",
    "O quinto _fold_ foi aquele que demonstrou maior dificuldade para os modelos ainda que o mesmo possuísse a mesmas características que os demais. Para tal _fold_ algums modelos chegaram a apresentar curvas lineares com algumas inflexões de função degrau.\n",
    "\n",
    "A partir da análise visual das curvas roc de cada classificador foi possível perceber que a __árvore de decisão__ obteve os melhores resultados, sendo o único classificador que não exibiu a curva do quinto _fold_ próxima a linha de orientação do gráfico.\n",
    "\n",
    "Quanto as métricas, aquele modelos que apresentou melhor acurácia de treino e, por consequente, melhor erro empírico foi o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_ein[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No que tange o erro esperado, o modelo que se sobressaiu foi o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_eout[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a precisão nas classes \"False Positive\" o modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_precision_false[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exibiu os mais apurados resultados. Já para a classe \"Confirmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_precision_true[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se destacou."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, temos a métrica revocação, nesta, para a classe \"False Positive\" o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_recall_false[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demonstrou os melhor resultados, enquanto o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_recall_true[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se sobressaiu para a classe \"Confirmed\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
